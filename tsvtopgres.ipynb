{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[TSV to Postgress DB](#toc0_)\n",
    "Features:\n",
    "\n",
    "- Unzipping gzip (gz) files using chunks and temporary files to prevent memory errors.\n",
    "- Determining column types for the tables.\n",
    "- Providing an option to alter DataFrame data to the proper list format for the PostgreSQL database when the column type is a list.\n",
    "- Creating Pandas DataFrames in chunks for efficient memory usage.\n",
    "- Uploading DataFrames as tables to a PostgreSQL database in chunks.\n",
    "- Offering an option to alter PostgreSQL table types.\n",
    "- Running a specific IMDb script that performs useful modifications to the IMDb database, including:\n",
    "    - Creating lookup tables (if not created during the database upload).\n",
    "    - Refining column data types.\n",
    "    - Remove rows from the lookup tables where the corresponding unique identifiers are no longer present in their respective tables.\n",
    "    - Adding primary and foreign keys to tables.\n",
    "    - Dropping the title_crew column and moving its contents to the lookup tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [TSV to Postgress DB](#toc1_1_)    \n",
    "      - [Initialization](#toc1_1_1_1_)    \n",
    "      - [Connect or create and connect to the Postgres DB](#toc1_1_1_2_)    \n",
    "      - [unzip gz file and read in chunks to avoid memory error](#toc1_1_1_3_)    \n",
    "      - [also temporary save of unzip file needed to avoid memory error](#toc1_1_1_4_)    \n",
    "      - [Create df with the tsv header only](#toc1_1_1_5_)    \n",
    "      - [Read random lines from the tsv_temp](#toc1_1_1_6_)    \n",
    "        - [Create random sample list with line numbers](#toc1_1_1_6_1_)    \n",
    "        - [Create df using the random_lines sample list](#toc1_1_1_6_2_)    \n",
    "      - [Copy df to ensure the original not be effected (just for experimental purpose)](#toc1_1_1_7_)    \n",
    "      - [Change the columns with the `list` type to a proper postgres list format](#toc1_1_1_8_)    \n",
    "      - [Check datatypes function](#toc1_1_1_9_)    \n",
    "      - [Create df with the cell types](#toc1_1_1_10_)    \n",
    "      - [Summerize the result which boils down to one data types for every columns](#toc1_1_1_11_)    \n",
    "      - [Map out a df with cell lengths](#toc1_1_1_12_)    \n",
    "      - [Create 2 rows in the temp_df to indicate the length of the cells](#toc1_1_1_13_)    \n",
    "      - [Final temp_df](#toc1_1_1_14_)    \n",
    "      - [Alter table query creator code](#toc1_1_1_15_)    \n",
    "      - [Code for change the postgress db table with the alter_query](#toc1_1_1_16_)    \n",
    "      - [so far read df in one go was ok](#toc1_1_1_17_)    \n",
    "      - [Insert df as chunks to db table to avoid memory error](#toc1_1_1_18_)    \n",
    "      - [Progress bar samples](#toc1_1_1_19_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_1_'></a>[Initialization](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For jupyter notebbok on VSC\n",
    "# %pip install --upgrade jupyter ipywidgets\n",
    "# %jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData, Table, text\n",
    "from sqlalchemy.exc import OperationalError\n",
    "from sqlalchemy_utils import create_database\n",
    "# from tqdm import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "tsv_directory = 'tsv'\n",
    "gz_directory = 'gz'\n",
    "\n",
    "# Check if the 'tsv' directory exists\n",
    "if not os.path.exists(tsv_directory):\n",
    "    print(f\"Directory '{tsv_directory}' does not exist.\")\n",
    "    quit()\n",
    "\n",
    "# username = input('Postgres username: ')\n",
    "# password = input('Postgres password: ')\n",
    "# host = input('Localhost: ')\n",
    "# port = input('Port: ')\n",
    "# dbase = input('Database: ')\n",
    "\n",
    "username = 'postgres'\n",
    "password = '123456'\n",
    "host = 'localhost'\n",
    "port = '5433'\n",
    "dbase = 'imdb_db'\n",
    "\n",
    "file_chunk_length = 1024*1024*25 # in bytes 1024*1024*500\n",
    "sample_size = 50 # in rows\n",
    "chunk_size = 10000 # in rows 200000\n",
    "\n",
    "rectify_data_types = 1\n",
    "pgres_list_rectif = 0\n",
    "drop_tables = True # if True earlier table with the same name will be dropped, False means append\n",
    "create_lookup = 1 # works only if rectify_data_types = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_2_'></a>[Connect or create and connect to the Postgres DB](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database: 'imdb_db'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    engine = create_engine(f'postgresql://{username}:{password}@{host}:{port}/{dbase}')\n",
    "    # Initialize MetaData\n",
    "    metadata = MetaData()\n",
    "    metadata.reflect(bind=engine)\n",
    "    print(f\"Connected to database: '{dbase}'.\")\n",
    "except OperationalError:\n",
    "    print(f\"Database '{dbase}' does not exist. Creating...\")\n",
    "    create_database(engine.url)\n",
    "    engine = create_engine(f'postgresql://{username}:{password}@{host}:{port}/{dbase}')\n",
    "    metadata = MetaData()\n",
    "    metadata.reflect(bind=engine)\n",
    "    print(f\"Connected to the created database: '{dbase}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_6_'></a>[Read random lines from the tsv_temp](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc1_1_1_6_1_'></a>[Create random sample list with line numbers](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(total_lines, sample_size): \n",
    "    random_lines = random.sample(range(1, total_lines), sample_size)\n",
    "    random_lines = [0] + random_lines\n",
    "    return random_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc1_1_1_6_2_'></a>[Create df using the random_lines sample list](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_df():\n",
    "    df = pd.read_csv(tsv_temp, sep='\\t', skiprows=lambda x: x not in random_lines, header=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_8_'></a>[Change the columns with the `list` type to a proper postgres list format](#toc0_)\n",
    "Measure required time too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgres_list_gen(df):\n",
    "    # start_t = time.time()\n",
    "    for col in (df_result.columns):\n",
    "        if df_result[col][0] == 'list':\n",
    "            df[col] = df[col].apply(lambda x: '{'+', '.join(f'\"{i}\"' for i in x.split(','))+'}' if x else None)\n",
    "        if df_result[col][0] == 'list[]':\n",
    "            df[col] = df[col].apply(lambda x: '{' + f'{x[1:-1]}' + '}' if x else None)\n",
    "    # end_t = time.time()\n",
    "    # print(f\"Execution time: {round((end_t - start_t), 4)} seconds\")\n",
    "    return df\n",
    "\n",
    "def change_str_to_list_in_df(df):\n",
    "    df = df.map(lambda x: None if x == '\\\\N' or x == 'NaN' else x)\n",
    "    start_t = time.time()\n",
    "    for col in (df_result.columns):\n",
    "        if df_result[col][0] == 'list':\n",
    "            df[col] = df[col].astype(str).str.split(',')\n",
    "        if df_result[col][0] == 'list[]':\n",
    "            df[col] = df[col].map(lambda x: [x[2:-2]] if x else None)\n",
    "    print(f\"Execution time: {round((end_t - start_t), 4)} seconds\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_9_'></a>[Check datatypes function](#toc0_)\n",
    "Checks datatypes of the df columns using the following logic:\n",
    "* checks if the cell is `\\N` or `NaN` alias empty cell and if it is return `None`\n",
    "* checks if the cell is a `string`,\n",
    "    - if ',' is in the str it considers as a `list`\n",
    "    - if the first character is a '-' it checks if it is a negativ  integer and if it is it marks as `integer-`\n",
    "else it checks if it a positiv integer\n",
    "    - tries if the str is a `float` if it is returns `floatnr` nr is the total decimals\n",
    "    - if all above guesses were `False` returns `string`\n",
    "* checks if the cell is an `integer`\n",
    "* checks if the cell is an `float` if it is returns the same as the str float check\n",
    "* checks if the cell is a `list`\n",
    "* if all the above failed tries to convert the cell type to `string` and check again everything\n",
    "* if all fails returns `nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_datatype(cell):\n",
    "    if cell:\n",
    "        if isinstance(cell, str):\n",
    "            if cell == '\\\\N' or cell == 'NaN':\n",
    "                return None\n",
    "            try:\n",
    "                if len(cell.split(',')) > 1 and len(cell.split(', ')) == 1:\n",
    "                    # return f'list_s{len(cell.split(','))}'\n",
    "\n",
    "                    return 'list'\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                if cell[0] == '[' and cell[-1] == ']':\n",
    "                    return 'list[]'\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                if cell[0] == '-':\n",
    "                    if cell[1:].isdigit():\n",
    "                        return 'integer-'\n",
    "                if cell.isdigit():\n",
    "                    return 'integer'\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                float(cell)\n",
    "                return f'float{len(str(cell).split('.')[1])}'\n",
    "            except:\n",
    "                pass\n",
    "            return 'string'\n",
    "        elif isinstance(cell, int):\n",
    "            return 'integer'\n",
    "        elif isinstance(cell, float):\n",
    "            if pd.isna(cell):\n",
    "                return None\n",
    "            else:\n",
    "                return f'float{len(str(cell).split('.')[1])}'\n",
    "        elif isinstance(cell, list):\n",
    "            return f'list{len(cell)}'\n",
    "        else:\n",
    "            try:\n",
    "                return check_datatype(str(cell))\n",
    "            except:\n",
    "                pass\n",
    "                return 'nan'  # Handle other data types\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_10_'></a>[Create df with the cell types](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_11_'></a>[Summerize the result which boils down to one data types for every columns](#toc0_)\n",
    "Logic:\n",
    "* if `None` is found in the list of types removes it\n",
    "* if `integer-` is found in the list changes the list to `[integer-]`, means that the column probably contains signed integers\n",
    "\n",
    "At the end the goal is to have a list with one element for all the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_12_'></a>[Map out a df with cell lengths](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_13_'></a>[Create 2 rows in the temp_df to indicate the length of the cells](#toc0_)\n",
    "* 1st row indicates if all cells are the same length `equal` or different `max`\n",
    "* 2nd row indicates the length of the cell, if the above cell in the same column is max it means indicates the `max` length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_14_'></a>[df_analysis](#toc0_)\n",
    "column names are the same as in the original df\n",
    "* 1st row shows the column data types\n",
    "* 2nd row is the indicator if the cell lengths are equal or not in the given column\n",
    "* 3rd indicates the length or max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_analysis():\n",
    "    df_result = pd.DataFrame(columns=df_lengths.columns).astype(str)\n",
    "    df_result.loc[len(df_result)] = ['a' for i in range(df_result.shape[1])]\n",
    "\n",
    "    for col in df_types.columns:\n",
    "        # if d_types[col].unique() != None:\n",
    "        unique_values = list(df_types[col].unique())\n",
    "        if len(unique_values) > 1:\n",
    "            for _ in unique_values:\n",
    "                if df_result.loc[0, col] != 'list[]':\n",
    "                    if _  == None:\n",
    "                        continue\n",
    "                    if _ == 'list[]':\n",
    "                        df_result.loc[0, col] = _\n",
    "                        break\n",
    "                    if df_result.loc[0, col] != 'list':\n",
    "                        if _ == 'list':\n",
    "                            df_result.loc[0, col] = _\n",
    "                            continue\n",
    "                        if 'float' not in df_result.loc[0, col]:\n",
    "                            if 'float' in _:\n",
    "                                df_result.loc[0, col] = _\n",
    "                                continue\n",
    "                            if df_result.loc[0, col] != 'integer-':\n",
    "                                if _ == 'integer-':\n",
    "                                    df_result.loc[0, col] = _\n",
    "                                    continue\n",
    "                                if df_result.loc[0, col] != 'integer':\n",
    "                                    if _ == 'integer':\n",
    "                                        df_result.loc[0, col] = _\n",
    "                                        continue\n",
    "                                    if _ == 'string':\n",
    "                                        df_result.loc[0, col] = _\n",
    "        else:\n",
    "            if unique_values[0] == None:\n",
    "                df_result.loc[0, col] = 'string'\n",
    "            else:\n",
    "                df_result.loc[0, col] = unique_values[0]\n",
    "\n",
    "    df_result.loc[len(df_result)] = ['equal' for i in range(df_result.shape[1])]\n",
    "    df_result.loc[len(df_result)] = [0 for i in range(df_result.shape[1])]\n",
    "\n",
    "    for column in df_lengths.columns:\n",
    "        # Find the maximum and minimum lengths in each column\n",
    "        max_length = df_lengths[column].max()\n",
    "        min_length = df_lengths[df_lengths[column] != 0][column].min()\n",
    "        \n",
    "        if max_length == 1:\n",
    "            list_unique = df_sample[col].unique()\n",
    "            if len(list_unique) == 2 and 0 in list_unique and 1 in list_unique:\n",
    "                df_result.loc[0, col] = 'bool'\n",
    "        elif max_length == min_length:\n",
    "            df_result.loc[2, column] = str(max_length)\n",
    "        elif not max_length:\n",
    "            df_result.loc[1, column] = 'NaN'\n",
    "            df_result.loc[2, column] = '10'\n",
    "        else:\n",
    "            df_result.loc[1, column] = 'max'\n",
    "            df_result.loc[2, column] = str(max_length)\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_15_'></a>[Alter table query creator code](#toc0_)\n",
    "Creates the Altering table query for postgres to alter the uploaded table columns' types based on the temp_df contents\n",
    "Logic:\n",
    "* `string` if `equal` be `VARCHAR(length + 2)` else `VARCHAR(max length + 100)`\n",
    "* `integer` if less than 5 digits be `SMALLINT` else `INT`\n",
    "* `list` be `VARCHAR[]`\n",
    "* `float` with any decimal places be `DOUBLE PRECISION`\n",
    "* `bool` will be `boolean`\n",
    "* none of the above be `TEXT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_query_gen_alter_col_types(df_result, table_name):\n",
    "    alter_query = f'ALTER TABLE {table_name} '\n",
    "    alter_query_2 = ''\n",
    "    for nr, col in enumerate(df_result.columns):\n",
    "        col_cells = df_result[col]\n",
    "        if col_cells[0] == 'string':\n",
    "            if col_cells[1] == 'equal':\n",
    "                col_type = f'VARCHAR({int(df_result[col][2])+2}) USING \"{col}\"::VARCHAR({int(df_result[col][2])+2})'\n",
    "            elif col_cells[1] =='NaN':\n",
    "                col_type = f'VARCHAR({int(df_result[col][2])}) USING \"{col}\"::VARCHAR({int(df_result[col][2])})'\n",
    "            else:\n",
    "                col_type = f'VARCHAR({int(df_result[col][2])+100}) USING \"{col}\"::VARCHAR({int(df_result[col][2])+100})'\n",
    "        elif col_cells[0] == 'integer':\n",
    "            if int(col_cells[2]) < 3:\n",
    "                col_type = f'SMALLINT USING \"{col}\"::smallint'\n",
    "            else:\n",
    "                col_type = f'INT USING \"{col}\"::integer'\n",
    "        elif 'list' in col_cells[0] and pgres_list_rectif:\n",
    "                col_type = f'VARCHAR[] USING \"{col}\"::varchar[]'\n",
    "        elif 'float' in col_cells[0]:\n",
    "            col_type = f'DOUBLE PRECISION USING \"{col}\"::double precision'\n",
    "        elif col_cells[0] == 'bool':\n",
    "            alter_query_2 = f'ALTER TABLE {table_name} ADD COLUMN new_boolean_column BOOLEAN;\\n\\\n",
    "                ALTER TABLE {table_name} ALTER COLUMN \"{col}\" TYPE INT USING \"{col}\"::integer;\\n\\\n",
    "                UPDATE {table_name} SET new_boolean_column = (\"{col}\" <> 0 AND \"isOriginalTitle\" IS NOT NULL);\\n\\\n",
    "                ALTER TABLE {table_name} DROP COLUMN \"{col}\";\\n\\\n",
    "                ALTER TABLE {table_name} RENAME COLUMN new_boolean_column TO \"{col}\";\\n'\n",
    "            col_type = ''\n",
    "            continue\n",
    "        else:\n",
    "            col_type = 'TEXT'\n",
    "        if nr == len(df_result.columns) - 1:\n",
    "            alter_query += f'ALTER COLUMN \"{col}\" TYPE {col_type}'\n",
    "        else:\n",
    "            alter_query += f'ALTER COLUMN \"{col}\" TYPE {col_type}, '\n",
    "\n",
    "    if alter_query_2:\n",
    "        alter_query = alter_query[:-2] + ';\\n' + alter_query_2\n",
    "        \n",
    "    return alter_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main cell to upload the tables to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "files = [f for f in os.listdir(tsv_directory) if f.endswith('.tsv')] or [f for f in os.listdir(tsv_directory) if f.endswith('.gz')] \n",
    "file_lines = {}\n",
    "if '.tsv' in files[0]:\n",
    "    try:\n",
    "        with open('app_data', 'r', encoding='utf-8') as file:\n",
    "            for _ in file.read().split('\\n')[:-1]:\n",
    "                file_lines[_.split()[0][:-1]] = int(_.split()[1])\n",
    "    except:\n",
    "        pass\n",
    "progress = tqdm(files, desc=f\"\", ncols=800)\n",
    "for _ in progress:\n",
    "    total_lines = 0\n",
    "    tsv_path = os.path.join(tsv_directory,_)\n",
    "    if _[-3:] == 'tsv':\n",
    "        tsv_temp = tsv_path\n",
    "        try:\n",
    "            total_lines = file_lines[tsv_temp]\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        tsv_temp = f\"{tsv_directory}/{_.rsplit('.', 1)[0]}\"\n",
    "    table_name = tsv_temp[4:-4].replace('.', '_')\n",
    "    progress.set_description(f\"Processing {table_name}, total lines: {total_lines} lines.\")\n",
    "    if table_name in metadata.tables and drop_tables:\n",
    "        Table(table_name, metadata, autoload_with=engine).drop(engine)\n",
    "        # print(f\"Table {table_name} dropped.\")\n",
    "    if not total_lines:\n",
    "        if _[-3:] == 'tsv':\n",
    "            with open(tsv_path, 'r', encoding='utf-8') as file:\n",
    "                while True:\n",
    "                    chunk = file.read(file_chunk_length)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    total_lines += chunk.count('\\n')\n",
    "                    progress.set_description(f\"Processing {table_name}, total lines: {total_lines} lines.\")\n",
    "        else:\n",
    "            with gzip.open(tsv_path, 'rb') as f:\n",
    "                f.seek(0, 2)  # Move to the end of the file\n",
    "                uncompressed_size = f.tell()\n",
    "            with gzip.open(tsv_path, 'rt', encoding='utf-8') as tsv_in, open(tsv_temp, 'w', encoding='utf-8') as tsv_out:\n",
    "                # while True:\n",
    "                total_nr_of_iter = uncompressed_size // file_chunk_length + 1\n",
    "                if uncompressed_size % file_chunk_length == 0:\n",
    "                    total_nr_of_iter -= 1\n",
    "                progress_1 = tqdm(range(total_nr_of_iter), desc=f\"Creating {tsv_temp} temp file total lines: {total_lines} lines.\", ncols=800)\n",
    "                for _ in progress_1:\n",
    "                    content = tsv_in.read(file_chunk_length)\n",
    "                    # progress_1.set_description(f\"Creating {tsv_temp}temp file\")\n",
    "                    total_lines += content.count('\\n')\n",
    "                    progress_1.set_description(f\"Creating {tsv_temp} temp file total lines: {total_lines} lines.\")\n",
    "                    progress.set_description(f\"Processing {table_name}, total lines: {total_lines} lines.\")\n",
    "                    tsv_out.write(content)\n",
    "    # rectify data types of the columns\n",
    "    if rectify_data_types:\n",
    "        # print(f\"Checking data types\")\n",
    "        random_lines = random_sampling(total_lines, sample_size)\n",
    "        df_sample = pd.read_csv(tsv_temp, sep='\\t', skiprows=lambda x: x not in random_lines, header=0)\n",
    "        df_types = df_sample.map(lambda x: (check_datatype(x)))\n",
    "        df_lengths = df_sample.map(lambda x: len(str(x)) if x not in (r'\\N', 'NaN') else 0)\n",
    "        df_result = df_analysis()\n",
    "\n",
    "    # Create a TextFileReader object using read_csv with the chunksize parameter\n",
    "    df_chunks = pd.read_csv(tsv_temp, sep='\\t', chunksize=chunk_size)\n",
    "\n",
    "    # Process each chunk\n",
    "    total_nr_of_iter = total_lines // chunk_size + 1\n",
    "    if  total_lines % chunk_size == 0:\n",
    "        total_nr_of_iter -= 1\n",
    "    progress_2 = tqdm(df_chunks, desc=f\"\", ncols=800, total=total_nr_of_iter)\n",
    "    check_is_done = 0\n",
    "\n",
    "    lookup_queries = {}\n",
    "    for df_chunk in progress_2:\n",
    "        progress_2.set_description(f\"Creating and uploading table in chunks of {chunk_size} rows.\")\n",
    "        # rectify list datatye (it is separated since it takes time) NOT WORKING\n",
    "        df_chunk = df_chunk.drop(df_chunk[df_chunk[df_chunk.columns[0]] == df_chunk.columns[0]].index)\n",
    "        if create_lookup and rectify_data_types:\n",
    "            lookups = {}\n",
    "            # df_result_lookup = pd.DataFrame()\n",
    "            for col in df_result.columns:\n",
    "                if df_result[col][0] == 'list':\n",
    "                    df_raw = df_chunk[col].str.split(',', expand=True).stack().reset_index(level=1, drop=True).rename(col)\n",
    "                    if 'nconst' in df_result:\n",
    "                        colm = 'nconst'\n",
    "                    if 'tconst' in df_result:\n",
    "                        colm = 'tconst'\n",
    "                    lookups[f'{col.lower()}_{table_name}_lookup'] = pd.merge(df_chunk[[colm]], df_raw, left_index=True, right_index=True).map(lambda x: None if x == '\\\\N' or x == 'NaN' else x)\n",
    "            for key, value in lookups.items():\n",
    "                value.to_sql(key, con=engine, if_exists='append', index=False)\n",
    "                if not check_is_done:\n",
    "                    random_lines = random_sampling(df_chunk.shape[0], sample_size)\n",
    "                    df_sample = value.iloc[random_lines]\n",
    "                    df_types = df_sample.map(lambda x: (check_datatype(x)))\n",
    "                    df_lengths = df_sample.map(lambda x: len(str(x)) if x not in (r'\\N', 'NaN') else 0)\n",
    "                    # if df_result_lookup.empty:\n",
    "                    df_result_lookup = df_analysis()\n",
    "                    lookup_queries[key] = db_query_gen_alter_col_types(df_result_lookup, key)\n",
    "                    # else:\n",
    "                    #     df_result_lookup[df_analysis().iloc[:, 1].name] = df_analysis().iloc[:, 1]\n",
    "            check_is_done = 1\n",
    "        df_chunk = df_chunk.map(lambda x: None if x == '\\\\N' or x == 'NaN' else x)\n",
    "        if pgres_list_rectif:\n",
    "            progress_2.set_description(f\"Rectifying list type in {table_name}\")\n",
    "            df_chunk = pgres_list_gen(df_chunk) # for test 5.74 it/s\n",
    "            # df_chunk = change_str_to_list_in_df(df_chunk) # for test 4.96 it/s\n",
    "        # insert df chunk\n",
    "        df_chunk.to_sql(table_name, con=engine, if_exists='append', index=False)\n",
    "        \n",
    "    if rectify_data_types:\n",
    "        progress.set_description(f\"Processing {table_name}, rectifying the data types of the columns.\")\n",
    "        for _ in tqdm(range(1), desc=f\"Rectifying col data types of {table_name} in database\", ncols=800):\n",
    "            alter_query = db_query_gen_alter_col_types(df_result, table_name)\n",
    "            # special case for title_basics in order to avoid 'invalid input syntax for type integer: \"Reality-TV\"'\n",
    "            if table_name == 'title_basics':\n",
    "                alter_query = alter_query.replace('ALTER COLUMN \"runtimeMinutes\" TYPE INT USING \"runtimeMinutes\"::integer',\n",
    "                                                'ALTER COLUMN \"runtimeMinutes\" TYPE VARCHAR(10) USING \"runtimeMinutes\"::VARCHAR(10)')\n",
    "            with engine.connect().execution_options(autocommit=True) as conn:\n",
    "                with conn.begin() as trans:\n",
    "                    conn.execute(text(alter_query))\n",
    "            if create_lookup and rectify_data_types:\n",
    "                for key, value in lookup_queries.items():\n",
    "                    with engine.connect().execution_options(autocommit=True) as conn:\n",
    "                        with conn.begin() as trans:\n",
    "                            conn.execute(text(value))\n",
    "    check_is_done = 0\n",
    "    file_lines[tsv_temp] = total_lines\n",
    "    # if tsv_temp == 'tsv\\\\s_title.basics.tsv':\n",
    "    #     sys.exit()\n",
    "    # if table_name == 's_title_akas':\n",
    "    #     sys.exit()\n",
    "    progress.set_description(f\"Process done.\")\n",
    "with open('app_data', 'w') as file:\n",
    "    for key, value in file_lines.items():\n",
    "        file.write(f'{key}: {value}\\n')\n",
    "\n",
    "# Disconnect the engine\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the `schema.sql` file against the database\n",
    "<p style=\"margin-top: -1rem;\">or any other SQL schema file.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the SQL script file (schema.sql, for example)\n",
    "with open('schema.sql', 'r') as file:\n",
    "    sql_script = file.read()\n",
    "\n",
    "# Execute the SQL script using the engine\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(text(sql_script))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sending a query to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"SELECT * FROM name_basics\"\n",
    "with engine.connect().execution_options(autocommit=True) as conn:\n",
    "    with conn.begin() as trans:\n",
    "        conn.execute(text(sql))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
